# ALL YEARS
# Facebook analysis, Harvard University
rm(list = ls()) # clean memory
# required packages
require(tidyverse)
require(dplyr)
require(mclust)
require(grdpg)
require(R.matlab)
require(igraph)
#### Data Preparation
# change next line to the folder containing the data
#datafolder <- "D:/grdpg_simulations/facebook100/" # research pc
#datafolder <- "/Users/Angelo/Dropbox/grdpg/paper/jasa_template/supplement/" # macbook 15
#datafolder <- "~/Dropbox/Angelo/" # macbook 15
datafolder <- "C:/Users/amele1/Dropbox/grdpg/paper/jasa_template/supplement/" # new reseaarch pc
# Harvard University data
file <- paste(datafolder, "Harvard1.mat", sep = "")
# TABLE FOR RESULTS
# columns are for elbow 1 and elbow 2
table_estimates <- data.frame(matrix(NA, ncol = 8, nrow = 10))
## Load data
data.network <- readMat(file)
## Adjacency matrix
A <- as.matrix(data.network$A)
## Nodes information
nodeinfo <- data.frame(data.network$local.info)
vblnames <- c("role", "gender", "major", "minor", "dorm", "year",  "highschool")
names(nodeinfo) <- vblnames
# count size
original_data_size <-dim(A)[1]
## descriptive statistics
g <- igraph::graph_from_adjacency_matrix(A, mode="undirected")
# avg degree
degree_orig <- igraph::degree(g)
avgdegree_orig <- mean(degree_orig)
# clustering
clust <- igraph::transitivity(g, type = "undirected")
# % female, % students, % off-campus
female_pct_orig <- sum(nodeinfo$gender==2)/original_data_size
student_pct_orig <- sum(nodeinfo$role==1)/original_data_size
offcampus_pct_orig <- sum(nodeinfo$dorm==0)/original_data_size
## Drop if gender is missing
missing.gender <- which(nodeinfo$gender==0)
A <- A[-missing.gender,-missing.gender]
nodeinfo <- nodeinfo[-missing.gender,]
# count dimension
lcc_year_gender_size <- dim(A)[1]
# Descriptive stats post-trimming
# count dimension
final_size <- dim(A)[1]
# keep only largest connected component
g <- igraph::graph_from_adjacency_matrix(A, mode="undirected")
components <- igraph::clusters(g)
biggest_cluster_id <- which.max(components$csize)
vert_ids <- igraph::V(g)[components$membership == biggest_cluster_id]
vert_largecomp <- as.numeric(vert_ids)
# take adjacency matrix and node info for largest component
A <- A[vert_largecomp,vert_largecomp]
nodeinfo <- nodeinfo[vert_largecomp,]
# count dimension
lcc_size <- dim(A)[1]
# # avg degree
# g <- igraph::graph_from_adjacency_matrix(A, mode="undirected")
# degree_lcc <- igraph::degree(g)
# avgdegree_lcc <- mean(degree_lcc)
# # clustering
# clust_lcc <- igraph::transitivity(g, type = "undirected")
# # % female, % students, % off-campus
# female_pct_lcc <- sum(nodeinfo$gender==2)/lcc_size
# student_pct_lcc <- sum(nodeinfo$role==1)/lcc_size
# offcampus_pct_lcc <- sum(nodeinfo$dorm==0)/lcc_size
#
# avg degree
g <- igraph::graph_from_adjacency_matrix(A, mode="undirected")
degree_final <- igraph::degree(g)
avgdegree_final <- mean(degree_final)
# clustering
clust_lcc <- igraph::transitivity(g, type = "undirected")
# % female, % students, % off-campus
female_pct_final <- sum(nodeinfo$gender==2)/final_size
student_pct_final <- sum(nodeinfo$role==1)/final_size
offcampus_pct_final <- sum(nodeinfo$dorm==0)/final_size
## TABLES WITH DESCRIPTIVE STATS
# original data
original_data_size
avgdegree_orig
female_pct_orig
student_pct_orig
offcampus_pct_orig
# largest connetd component
# lcc_size
# avgdegree_lcc
# female_pct_lcc
# student_pct_lcc
# offcampus_pct_lcc
# final network
final_size
avgdegree_final
female_pct_final
student_pct_final
offcampus_pct_final
###### MODELING ANALYSIS
## regularization
# compute average degree
avgdegree <- sum(A)/(dim(A)[1])
A_orig <- A                               # keep original adjacency matrix
diag(A) <- diag(A)+ rowSums(A)/dim(A)[1]  # diagonal is augmented with avg degree/n
A <- A + avgdegree/dim(A)[1]              # Levina et al regularization for sparse graphs
# modify the role variable to have only 2 values (student or not student)
# otherwise there are too few observations in some blocks
nodeinfo[nodeinfo$role>1,1] <- 2
# rewrite dorm as in campus/off-campus
nodeinfo[nodeinfo$dorm>0,5] <- 2
nodeinfo[nodeinfo$dorm==0,5] <- 1
## ANALYSIS WITH MULTIPLE COVARIATES (3 binary covariates)
# gender and role
cov <-c(2,2,2)
covariates <- matrix(as.matrix(nodeinfo[,c(2,1,5)]), ncol = 3)
#### GRDPG with Covariates for Facebook Data
dmax <- 40
## ASE
set.seed(1977)
embed <- SpectralEmbedding(A, dmax, work = 100)
s <- embed$D
## Choose embed dimension, use first elbow (Zhu and Ghodsi 2006)
dimselect(s)
require(doParallel)
registerDoParallel(cores = 8)
df_res <- foreach (K = 2:5, .combine='rbind') %:%
foreach (elb = 1:2, .combine='rbind') %dopar% {
cat(paste("K=",K," ; elbow=",elb,sep=""), "\n")
G <- K*prod(cov)
dhat <- dimselect(s)$elbow[elb]+1
## Estimate latent position
Yhat <- embed$X[,1:dhat] %*% sqrt(diag(s[1:dhat], nrow=dhat, ncol=dhat))
Ipq <- getIpq(A, dhat)
## Cluster based on estimated latent position
set.seed(1977)
model <- Mclust(Yhat, G)
muhats <- model$parameters$mean
BXhat <- logit(BFcheck(t(muhats) %*% Ipq %*% muhats))
## Estimate beta
clusters_cov <- getClusters(data.frame(model$z))
covariates_block <- getBlockCovariates(covariates, clusters_cov)
check = "BF"
# Use weighted estimator
result2 <- estimatebeta_WA(Yhat, muhats, Ipq, cov, covariates, clusters_cov, link = 'logit', sd = FALSE, check = check)
betahat2 <- sapply(Map('*',result2$betahats,result2$pis), sum)
# betahat2_unbiased <- betahat2 - sapply(Map('*',result2$bias,result2$pis), sum)
# varweights <- Map('*', result2$pis, result2$pis)
# var2 <- sapply(Map('*',result2$sd2s,varweights), sum)
# sd2 <- sqrt(var2)/dim(A)[1]
# if (elb == 1){
#   table_estimates[K-1,1] <- dhat
#   table_estimates[K-1,2:4] <- betahat2
# } else {
#   table_estimates[K-1,5] <- dhat
#   table_estimates[K-1,6:8] <- betahat2
# }
df <- tibble(K=K, G=G, dhat=dhat, Yhat=list(Yhat), Ipq=list(Ipq), model=list(model), betahat = list(betahat2))
save(df, file="df_all_year-MA.RData")
df
}
table_estimates
save(df_res, file="df_res_all_year-MA.RData")
file_estimates <- paste(datafolder, "Harvard_checksign2.Rdata", sep = "")
save.image(file = file_estimates )
####################
# ALL YEARS
# Facebook analysis, Harvard University
rm(list = ls()) # clean memory
# required packages
require(tidyverse)
require(dplyr)
require(mclust)
require(grdpg)
require(R.matlab)
require(igraph)
#### Data Preparation
# change next line to the folder containing the data
#datafolder <- "D:/grdpg_simulations/facebook100/" # research pc
#datafolder <- "/Users/Angelo/Dropbox/grdpg/paper/jasa_template/supplement/" # macbook 15
#datafolder <- "~/Dropbox/Angelo/" # macbook 15
datafolder <- "C:/Users/amele1/Dropbox/grdpg/paper/jasa_template/supplement/" # new reseaarch pc
# Harvard University data
file <- paste(datafolder, "Harvard1.mat", sep = "")
# TABLE FOR RESULTS
# columns are for elbow 1 and elbow 2
table_estimates <- data.frame(matrix(NA, ncol = 8, nrow = 10))
## Load data
data.network <- readMat(file)
## Adjacency matrix
A <- as.matrix(data.network$A)
## Nodes information
nodeinfo <- data.frame(data.network$local.info)
vblnames <- c("role", "gender", "major", "minor", "dorm", "year",  "highschool")
names(nodeinfo) <- vblnames
# count size
original_data_size <-dim(A)[1]
## descriptive statistics
g <- igraph::graph_from_adjacency_matrix(A, mode="undirected")
# avg degree
degree_orig <- igraph::degree(g)
avgdegree_orig <- mean(degree_orig)
# clustering
clust <- igraph::transitivity(g, type = "undirected")
# % female, % students, % off-campus
female_pct_orig <- sum(nodeinfo$gender==2)/original_data_size
student_pct_orig <- sum(nodeinfo$role==1)/original_data_size
offcampus_pct_orig <- sum(nodeinfo$dorm==0)/original_data_size
## Drop if gender is missing
missing.gender <- which(nodeinfo$gender==0)
A <- A[-missing.gender,-missing.gender]
nodeinfo <- nodeinfo[-missing.gender,]
# count dimension
lcc_year_gender_size <- dim(A)[1]
# Descriptive stats post-trimming
# count dimension
final_size <- dim(A)[1]
# keep only largest connected component
g <- igraph::graph_from_adjacency_matrix(A, mode="undirected")
components <- igraph::clusters(g)
biggest_cluster_id <- which.max(components$csize)
vert_ids <- igraph::V(g)[components$membership == biggest_cluster_id]
vert_largecomp <- as.numeric(vert_ids)
# take adjacency matrix and node info for largest component
A <- A[vert_largecomp,vert_largecomp]
nodeinfo <- nodeinfo[vert_largecomp,]
# count dimension
lcc_size <- dim(A)[1]
# # avg degree
# g <- igraph::graph_from_adjacency_matrix(A, mode="undirected")
# degree_lcc <- igraph::degree(g)
# avgdegree_lcc <- mean(degree_lcc)
# # clustering
# clust_lcc <- igraph::transitivity(g, type = "undirected")
# # % female, % students, % off-campus
# female_pct_lcc <- sum(nodeinfo$gender==2)/lcc_size
# student_pct_lcc <- sum(nodeinfo$role==1)/lcc_size
# offcampus_pct_lcc <- sum(nodeinfo$dorm==0)/lcc_size
#
# avg degree
g <- igraph::graph_from_adjacency_matrix(A, mode="undirected")
degree_final <- igraph::degree(g)
avgdegree_final <- mean(degree_final)
# clustering
clust_lcc <- igraph::transitivity(g, type = "undirected")
# % female, % students, % off-campus
female_pct_final <- sum(nodeinfo$gender==2)/final_size
student_pct_final <- sum(nodeinfo$role==1)/final_size
offcampus_pct_final <- sum(nodeinfo$dorm==0)/final_size
## TABLES WITH DESCRIPTIVE STATS
# original data
original_data_size
avgdegree_orig
female_pct_orig
student_pct_orig
offcampus_pct_orig
# largest connetd component
# lcc_size
# avgdegree_lcc
# female_pct_lcc
# student_pct_lcc
# offcampus_pct_lcc
# final network
final_size
avgdegree_final
female_pct_final
student_pct_final
offcampus_pct_final
###### MODELING ANALYSIS
## regularization
# compute average degree
avgdegree <- sum(A)/(dim(A)[1])
A_orig <- A                               # keep original adjacency matrix
diag(A) <- diag(A)+ rowSums(A)/dim(A)[1]  # diagonal is augmented with avg degree/n
A <- A + avgdegree/dim(A)[1]              # Levina et al regularization for sparse graphs
# modify the role variable to have only 2 values (student or not student)
# otherwise there are too few observations in some blocks
nodeinfo[nodeinfo$role>1,1] <- 2
# rewrite dorm as in campus/off-campus
nodeinfo[nodeinfo$dorm>0,5] <- 2
nodeinfo[nodeinfo$dorm==0,5] <- 1
## ANALYSIS WITH MULTIPLE COVARIATES (3 binary covariates)
# gender and role
cov <-c(2,2,2)
covariates <- matrix(as.matrix(nodeinfo[,c(2,1,5)]), ncol = 3)
#### GRDPG with Covariates for Facebook Data
dmax <- 40
## ASE
set.seed(1977)
embed <- SpectralEmbedding(A, dmax, work = 100)
s <- embed$D
## Choose embed dimension, use first elbow (Zhu and Ghodsi 2006)
dimselect(s)
require(doParallel)
registerDoParallel(cores = 8)
df_res <- foreach (K = 2:5, .combine='rbind') %:%
foreach (elb = 1:2, .combine='rbind') %dopar% {
cat(paste("K=",K," ; elbow=",elb,sep=""), "\n")
library(grdpg)
G <- K*prod(cov)
dhat <- dimselect(s)$elbow[elb]+1
## Estimate latent position
Yhat <- embed$X[,1:dhat] %*% sqrt(diag(s[1:dhat], nrow=dhat, ncol=dhat))
Ipq <- getIpq(A, dhat)
## Cluster based on estimated latent position
set.seed(1977)
model <- Mclust(Yhat, G)
muhats <- model$parameters$mean
BXhat <- logit(BFcheck(t(muhats) %*% Ipq %*% muhats))
## Estimate beta
clusters_cov <- getClusters(data.frame(model$z))
covariates_block <- getBlockCovariates(covariates, clusters_cov)
check = "BF"
# Use weighted estimator
result2 <- estimatebeta_WA(Yhat, muhats, Ipq, cov, covariates, clusters_cov, link = 'logit', sd = FALSE, check = check)
betahat2 <- sapply(Map('*',result2$betahats,result2$pis), sum)
# betahat2_unbiased <- betahat2 - sapply(Map('*',result2$bias,result2$pis), sum)
# varweights <- Map('*', result2$pis, result2$pis)
# var2 <- sapply(Map('*',result2$sd2s,varweights), sum)
# sd2 <- sqrt(var2)/dim(A)[1]
# if (elb == 1){
#   table_estimates[K-1,1] <- dhat
#   table_estimates[K-1,2:4] <- betahat2
# } else {
#   table_estimates[K-1,5] <- dhat
#   table_estimates[K-1,6:8] <- betahat2
# }
df <- tibble(K=K, G=G, dhat=dhat, Yhat=list(Yhat), Ipq=list(Ipq), model=list(model), betahat = list(betahat2))
save(df, file="df_all_year-MA.RData")
df
}
table_estimates
save(df_res, file="df_res_all_year-MA.RData")
file_estimates <- paste(datafolder, "Harvard_checksign2.Rdata", sep = "")
save.image(file = file_estimates )
####################
# ALL YEARS
# Facebook analysis, Harvard University
rm(list = ls()) # clean memory
# required packages
require(tidyverse)
require(dplyr)
require(mclust)
require(grdpg)
require(R.matlab)
require(igraph)
#### Data Preparation
# change next line to the folder containing the data
#datafolder <- "D:/grdpg_simulations/facebook100/" # research pc
#datafolder <- "/Users/Angelo/Dropbox/grdpg/paper/jasa_template/supplement/" # macbook 15
#datafolder <- "~/Dropbox/Angelo/" # macbook 15
datafolder <- "C:/Users/amele1/Dropbox/grdpg/paper/jasa_template/supplement/" # new reseaarch pc
# Harvard University data
file <- paste(datafolder, "Harvard1.mat", sep = "")
# TABLE FOR RESULTS
# columns are for elbow 1 and elbow 2
table_estimates <- data.frame(matrix(NA, ncol = 8, nrow = 10))
## Load data
data.network <- readMat(file)
## Adjacency matrix
A <- as.matrix(data.network$A)
## Nodes information
nodeinfo <- data.frame(data.network$local.info)
vblnames <- c("role", "gender", "major", "minor", "dorm", "year",  "highschool")
names(nodeinfo) <- vblnames
# count size
original_data_size <-dim(A)[1]
## descriptive statistics
g <- igraph::graph_from_adjacency_matrix(A, mode="undirected")
# avg degree
degree_orig <- igraph::degree(g)
avgdegree_orig <- mean(degree_orig)
# clustering
clust <- igraph::transitivity(g, type = "undirected")
# % female, % students, % off-campus
female_pct_orig <- sum(nodeinfo$gender==2)/original_data_size
student_pct_orig <- sum(nodeinfo$role==1)/original_data_size
offcampus_pct_orig <- sum(nodeinfo$dorm==0)/original_data_size
## Drop if gender is missing
missing.gender <- which(nodeinfo$gender==0)
A <- A[-missing.gender,-missing.gender]
nodeinfo <- nodeinfo[-missing.gender,]
# count dimension
lcc_year_gender_size <- dim(A)[1]
# Descriptive stats post-trimming
# count dimension
final_size <- dim(A)[1]
# keep only largest connected component
g <- igraph::graph_from_adjacency_matrix(A, mode="undirected")
components <- igraph::clusters(g)
biggest_cluster_id <- which.max(components$csize)
vert_ids <- igraph::V(g)[components$membership == biggest_cluster_id]
vert_largecomp <- as.numeric(vert_ids)
# take adjacency matrix and node info for largest component
A <- A[vert_largecomp,vert_largecomp]
nodeinfo <- nodeinfo[vert_largecomp,]
# count dimension
lcc_size <- dim(A)[1]
# # avg degree
# g <- igraph::graph_from_adjacency_matrix(A, mode="undirected")
# degree_lcc <- igraph::degree(g)
# avgdegree_lcc <- mean(degree_lcc)
# # clustering
# clust_lcc <- igraph::transitivity(g, type = "undirected")
# # % female, % students, % off-campus
# female_pct_lcc <- sum(nodeinfo$gender==2)/lcc_size
# student_pct_lcc <- sum(nodeinfo$role==1)/lcc_size
# offcampus_pct_lcc <- sum(nodeinfo$dorm==0)/lcc_size
#
# avg degree
g <- igraph::graph_from_adjacency_matrix(A, mode="undirected")
degree_final <- igraph::degree(g)
avgdegree_final <- mean(degree_final)
# clustering
clust_lcc <- igraph::transitivity(g, type = "undirected")
# % female, % students, % off-campus
female_pct_final <- sum(nodeinfo$gender==2)/final_size
student_pct_final <- sum(nodeinfo$role==1)/final_size
offcampus_pct_final <- sum(nodeinfo$dorm==0)/final_size
## TABLES WITH DESCRIPTIVE STATS
# original data
original_data_size
avgdegree_orig
female_pct_orig
student_pct_orig
offcampus_pct_orig
# largest connetd component
# lcc_size
# avgdegree_lcc
# female_pct_lcc
# student_pct_lcc
# offcampus_pct_lcc
# final network
final_size
avgdegree_final
female_pct_final
student_pct_final
offcampus_pct_final
###### MODELING ANALYSIS
## regularization
# compute average degree
avgdegree <- sum(A)/(dim(A)[1])
A_orig <- A                               # keep original adjacency matrix
diag(A) <- diag(A)+ rowSums(A)/dim(A)[1]  # diagonal is augmented with avg degree/n
A <- A + avgdegree/dim(A)[1]              # Levina et al regularization for sparse graphs
# modify the role variable to have only 2 values (student or not student)
# otherwise there are too few observations in some blocks
nodeinfo[nodeinfo$role>1,1] <- 2
# rewrite dorm as in campus/off-campus
nodeinfo[nodeinfo$dorm>0,5] <- 2
nodeinfo[nodeinfo$dorm==0,5] <- 1
## ANALYSIS WITH MULTIPLE COVARIATES (3 binary covariates)
# gender and role
cov <-c(2,2,2)
covariates <- matrix(as.matrix(nodeinfo[,c(2,1,5)]), ncol = 3)
#### GRDPG with Covariates for Facebook Data
dmax <- 40
## ASE
set.seed(1977)
embed <- SpectralEmbedding(A, dmax, work = 100)
s <- embed$D
## Choose embed dimension, use first elbow (Zhu and Ghodsi 2006)
dimselect(s)
require(doParallel)
registerDoParallel(cores = 8)
df_res <- foreach (K = 2:5, .combine='rbind') %:%
foreach (elb = 1:2, .combine='rbind') %dopar% {
cat(paste("K=",K," ; elbow=",elb,sep=""), "\n")
library(grdpg)
library(mclust)
library(tidyverse)
G <- K*prod(cov)
dhat <- dimselect(s)$elbow[elb]+1
## Estimate latent position
Yhat <- embed$X[,1:dhat] %*% sqrt(diag(s[1:dhat], nrow=dhat, ncol=dhat))
Ipq <- getIpq(A, dhat)
## Cluster based on estimated latent position
set.seed(1977)
model <- Mclust(Yhat, G)
muhats <- model$parameters$mean
BXhat <- logit(BFcheck(t(muhats) %*% Ipq %*% muhats))
## Estimate beta
clusters_cov <- getClusters(data.frame(model$z))
covariates_block <- getBlockCovariates(covariates, clusters_cov)
check = "BF"
# Use weighted estimator
result2 <- estimatebeta_WA(Yhat, muhats, Ipq, cov, covariates, clusters_cov, link = 'logit', sd = FALSE, check = check)
betahat2 <- sapply(Map('*',result2$betahats,result2$pis), sum)
# betahat2_unbiased <- betahat2 - sapply(Map('*',result2$bias,result2$pis), sum)
# varweights <- Map('*', result2$pis, result2$pis)
# var2 <- sapply(Map('*',result2$sd2s,varweights), sum)
# sd2 <- sqrt(var2)/dim(A)[1]
# if (elb == 1){
#   table_estimates[K-1,1] <- dhat
#   table_estimates[K-1,2:4] <- betahat2
# } else {
#   table_estimates[K-1,5] <- dhat
#   table_estimates[K-1,6:8] <- betahat2
# }
df <- tibble(K=K, G=G, dhat=dhat, Yhat=list(Yhat), Ipq=list(Ipq), model=list(model), betahat = list(betahat2))
save(df, file="df_all_year-MA.RData")
df
}
table_estimates
save(df_res, file="df_res_all_year-MA.RData")
file_estimates <- paste(datafolder, "Harvard_checksign2.Rdata", sep = "")
save.image(file = file_estimates )
library(devtools)
install_github("meleangelo/grdpg")
